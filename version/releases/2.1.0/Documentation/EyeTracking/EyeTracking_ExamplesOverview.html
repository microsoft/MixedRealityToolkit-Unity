<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Eye tracking examples in MRTK | Mixed Reality Toolkit Documentation </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Eye tracking examples in MRTK | Mixed Reality Toolkit Documentation ">
    <meta name="generator" content="docfx 2.45.1.0">
    
    <link rel="shortcut icon" href="../../Documentation/Images/favicon.ico">
    <link rel="stylesheet" href="../../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../../styles/docfx.css">
    <link rel="stylesheet" href="../../styles/main.css">
    <meta property="docfx:navrel" content="../../toc.html">
    <meta property="docfx:tocrel" content="../toc.html">
    
    <meta property="docfx:rel" content="../../">
    
  </head>
  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../../index.html">
                <img id="logo" class="svg" src="../../Documentation/Images/mrt_logo_icon.png" alt="">
              </a>
            </div>
          
          <div class="version-dropdown" id="versionDropdown">
           </div>
         
          <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div class="container body-content">
        
        <div id="search-results">
          <div class="search-list"></div>
          <div class="sr-items">
            <p><i class="glyphicon glyphicon-refresh index-loading"></i></p>
          </div>
          <ul id="pagination"></ul>
        </div>
      </div>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="">
<h1 id="eye-tracking-examples-in-mrtk">Eye tracking examples in MRTK</h1>

<p>This page covers how to get quickly started with using eye tracking in MRTK by building on our provided <a href="https://github.com/Microsoft/MixedRealityToolkit-Unity/tree/mrtk_release/Assets/MixedRealityToolkit.Examples/Demos/EyeTracking">MRTK eye tracking examples</a>.
The samples let you experience one of our new magical input capabilities: <strong>Eye tracking</strong>!
The demo includes a number of different use cases ranging from implicit eye-based activations to how to seamlessly combine information about what you are looking at with <strong>voice</strong> and <strong>hand</strong> input.
This enables users to quickly and effortlessly select and move holographic content across their view simply by looking at a target and saying <em>'Select'</em> or performing a hand gesture.
The demos also include an example for eye-gaze-directed scroll, pan and zoom of text and images on a slate.
Finally, an example is provided for recording and visualizing the user's visual attention on a 2D slate.
In the following, we will go into more detail what each of the different samples in the <a href="https://github.com/Microsoft/MixedRealityToolkit-Unity/tree/mrtk_release/Assets/MixedRealityToolkit.Examples/Demos/EyeTracking">MRTK eye tracking example package</a> includes:</p>
<p><img src="../Images/EyeTracking/mrtk_et_list_et_scenes.jpg" alt="List of eye tracking scenes"></p>
<p>We will start with a quick overview of what the individual eye tracking demo scenes are about.
The MRTK eye tracking demo scenes are <a href="https://docs.unity3d.com/ScriptReference/SceneManagement.LoadSceneMode.Additive.html">loaded additively</a> which we will explain below how to set up.</p>
<h2 id="overview-of-the-eye-tracking-demo-samples">Overview of the eye tracking demo samples</h2>
<h3 id="eye-supported-target-selection"><a href="EyeTracking_TargetSelection.html"><strong>Eye-Supported Target Selection</strong></a></h3>
<p>This tutorial showcases the ease of accessing eye gaze data to select targets.
It includes an example for subtle yet powerful feedback to provide confidence to the user that a target is focused while not being overwhelming.
In addition, there is a simple example of smart notifications that automatically disappear after being read.</p>
<p><strong>Summary</strong>: Fast and effortless target selections using a combination of eyes, voice and hand input.</p>
<br>
<h3 id="eye-supported-navigation"><a href="EyeTracking_Navigation.html"><strong>Eye-Supported Navigation</strong></a></h3>
<p>Imagine you are reading some information on a distant display or your e-reader and when you reach the end of the displayed text, the text automatically scrolls up to reveal more content.
Or how about magically zooming directly toward where you were looking at?
These are some of the examples showcased in this tutorial about eye-supported navigation.
In addition, there is an example for hands-free rotation of 3D holograms by making them automatically rotate based on your current focus.</p>
<p><strong>Summary</strong>: Scroll, pan, zoom, 3D rotation using a combination of eyes, voice and hand input.</p>
<br>
<h3 id="eye-supported-positioning"><a href="EyeTracking_Positioning.html"><strong>Eye-Supported Positioning</strong></a></h3>
<p>This tutorial shows an input scenario called <a href="https://youtu.be/CbIn8p4_4CQ">Put-That-There</a> dating back to research from the MIT Media Lab in the early 1980's with eye, hand and voice input.
The idea is simple: Benefit from your eyes for fast target selection and positioning.
Simply look at a hologram and say <em>'put this'</em>, look over where you want to place it and say <em>'there!'</em>.
For more precisely positioning your hologram, you can use additional input from your hands, voice or controllers.</p>
<p><strong>Summary</strong>: Positioning holograms using eyes, voice and hand input (<em>drag-and-drop</em>). Eye-supported sliders using eyes + hands.</p>
<br>
<h3 id="visualization-of-visual-attention"><strong>Visualization of Visual Attention</strong></h3>
<p>Information about where users look at is an immensely powerful tool to assess usability of a design and to identify problems in efficient work streams.
This tutorial discusses different eye tracking visualizations and how they fit different needs.
We provide basic examples for logging and loading eye tracking data and examples for how to visualize them.</p>
<p><strong>Summary</strong>: Two-dimensional attention map (heatmaps) on slates. Recording &amp; replaying eye tracking data.</p>
<br>
<h2 id="setting-up-the-mrtk-eye-tracking-samples">Setting up the MRTK eye tracking samples</h2>
<h3 id="prerequisites">Prerequisites</h3>
<p>Note that using the eye tracking samples on device requires a HoloLens 2
and a sample app package that is built with the &quot;Gaze Input&quot; capability
on the package's AppXManifest.</p>
<p>In order to use these eye tracking samples on device, make sure to follow
<a href="EyeTracking_BasicSetup.html#testing-your-unity-app-on-a-hololens-2">these steps</a>
prior to building the app in Visual Studio.</p>
<h3 id="1-load-eyetrackingdemo-00-rootsceneunity">1. Load EyeTrackingDemo-00-RootScene.unity</h3>
<p>The <em>EyeTrackingDemo-00-RootScene</em> is the base (<em>root</em>) scene that has all the core MRTK components included.
This is the scene that you need to load first and from which you will run the eye tracking demos.
It comes with a graphical scene menu that allows you to easily switch between the different eye tracking samples which will be <a href="https://docs.unity3d.com/ScriptReference/SceneManagement.LoadSceneMode.Additive.html">loaded additively</a>.</p>
<p><img src="../Images/EyeTracking/mrtk_et_scenemenu.jpg" alt="Scene menu in eye tracking sample"></p>
<p>The root scene includes a few core components that will persist across the additively loaded scenes, such as the MRTK configured profiles and scene camera.
The <em>MixedRealityBasicSceneSetup</em> (see screenshot below) includes a script that will automatically load the referenced scene on startup.
By default this is <em>EyeTrackingDemo-02-TargetSelection</em>.</p>
<p><img src="../Images/EyeTracking/mrtk_et_onloadstartscene.jpg" alt="Example for the OnLoadStartScene script"></p>
<h3 id="2-adding-scenes-to-the-build-menu">2. Adding scenes to the Build menu</h3>
<p>To load additive scenes during runtime, you must add these scenes to your <em>Build Settings -&gt; Scenes in Build</em> menu first.
It is important that the root scene is shown as the first scene in the list:</p>
<p><img src="../Images/EyeTracking/mrtk_et_build_settings.jpg" alt="Build Settings scene menu for eye tracking samples"></p>
<h3 id="3-play-the-eye-tracking-samples-in-the-unity-editor">3. Play the eye tracking samples in the Unity Editor</h3>
<p>After adding the eye tracking scenes to the Build Settings and loading the <em>EyeTrackingDemo-00-RootScene</em>, there is one last thing you may want to check: Is the <em>'OnLoadStartScene'</em> script that is attached to the <em>MixedRealityBasicSceneSetup</em> GameObject enabled? This is to let the root scene know which demo scene to load first.</p>
<p><img src="../Images/EyeTracking/mrtk_et_onloadstartscene.jpg" alt="Example for the OnLoad_StartScene script"></p>
<p>Let's roll! Hit <em>&quot;Play&quot;</em>!
You should see several gems appear and should see the scene menu at the top.</p>
<p><img src="../Images/EyeTracking/mrtk_et_targetselect.png" alt="Sample screenshot from the ET target select scene"></p>
<p>You should notice a small semitransparent circle at the center of your game view.
This acts as an indicator (cursor) of your <em>simulated eye gaze</em>:
Simply press down the <em>right mouse button</em> and move the mouse to change its position.
When the cursor is hovering over the gems, you will notice that it will snap to the center of the currently looked at gem.
This is a great way to test if events are triggered as expected when <em>&quot;looking&quot;</em> at a target.
Please beware that the <em>simulated eye gaze</em> via mouse control is a rather poor supplement to our rapid and unintentional eye movements.
It is great for testing the basic functionality though before iterating on the design by deploying it to the HoloLens 2 device.
Coming back to our eye tracking sample scene: The gem rotates as long as being looked at and can be destroyed by &quot;looking&quot; at it and ...</p>
<ul>
<li>Pressing <em>Enter</em> (which simulates saying &quot;select&quot;)</li>
<li>Saying <em>&quot;select&quot;</em> into your microphone</li>
<li>While pressing <em>Space</em> to show the simulated hand input, click the left mouse button to perform a simulated pinch</li>
</ul>
<p>We describe in more detail how you can achieve these interactions in our <a href="EyeTracking_TargetSelection.html"><strong>Eye-Supported Target Selection</strong></a> tutorial.</p>
<p>When moving the cursor up to the top menu bar in the scene, you will notice that the currently hovered item will highlight subtly.
You can select the currently highlighted item by using one of the above described commit methods (e.g., pressing <em>Enter</em>).
This way you can switch between the different eye tracking sample scenes.</p>
<h3 id="4-how-to-test-specific-sub-scenes">4. How to test specific sub scenes</h3>
<p>When working on a specific scenario, you may not want to go through the scene menu every time.
Instead you may want to be able to start out directly from the scene that you are currently working on when pressing the <em>Play</em> button.
No problem! Here is what you can do:</p>
<ol>
<li>Load the <em>root</em> scene</li>
<li>In the <em>root</em> scene, disable the <em>'OnLoadStartScene'</em> script</li>
<li><em>Drag and drop</em> one of the eye tracking test scenes that are described below (or any other scene) into your <em>Hierarchy</em> view as shown in the screenshot below.</li>
</ol>
<p><img src="../Images/EyeTracking/mrtk_et_additivescene.jpg" alt="Example for additive scene"></p>
<ol start="4">
<li>Press <em>Play</em></li>
</ol>
<p>Please note that loading the sub scene like this is not persistent:
This means that if you deploy your app to the HoloLens 2 device, it will only load the root scene (assuming it appears at the top of your Build Settings).
Also, when you share your project with others, the sub scenes are not automatically loaded.</p>
<br>
<p>Now that you know how to get the MRTK eye tracking example scenes to work, let's continue with diving deeper into how to select holograms with your eyes: <a href="EyeTracking_TargetSelection.html">Eye-supported target selection</a>.</p>
<hr>
<p><a href="EyeTracking_Main.html">Back to &quot;Eye tracking in the MixedRealityToolkit&quot;</a></p>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                  <li>
                    <a href="https://github.com/Microsoft/MixedRealityToolkit-Unity/blob/mrtk_development/Documentation/EyeTracking/EyeTracking_ExamplesOverview.md/#L1" class="contribution-link">Improve this Doc</a>
                  </li>
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            
            <span>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../../styles/docfx.js"></script>
    <script type="text/javascript" src="../../styles/main.js"></script>
  </body>
</html>
